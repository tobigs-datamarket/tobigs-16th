{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8847qhBzpV3"
   },
   "source": [
    "# Week6: Class\n",
    "\n",
    "Pytorch의 Dataset 추상클래스를 상속받아 CustomDataset 만들어보기!\n",
    "\n",
    "- 현재는 X_data, y_data를 임의로 선정해두었지만, 원한다면 어떠한 데이터를 쓰셔도 상관없습니다. \n",
    "\n",
    "- 선언되어있는 3가지 메서드는 모두 구현하셔야 합니다. \n",
    "- 연습을 위한 추가적인 메서드들은 언제나 환영합니다!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preface\n",
    "많은 양의 data를 이용해서 딥러닝 모델을 학습시키는 일이 많아지면서 그 많은 양의 data를 한번에 불러오려면 시간이 오래걸리는 것을 넘어서서 data의 크기가 RAM용량을 넘을경우 메모리 관리에 오버헤드가 커져 비효율적인 학습이 이뤄질 수 있다. 따라서 데이터를 한번에 다 부르지 않고 하나씩만 불러서 쓰는 방식을 택하면 쾌적하게 모델을 돌릴 수 있다. 그래서 모든 데이터를 한번에 불러놓고 쓰는 기존의 dataset말고 custom dataset을 만들어야할 필요가 있다. 또한 길이가 변하는 input에 대해서 batch를 만들기 위해서는 dataloader에서 batch를 만드는 부분을 수정해야할 필요가 있어 custom dataloader를 사용해야 한다. 파이토치에서는 데이터를 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 데이터셋(`Dataset`)과 데이터로더(`DataLoader`)를 제공한다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다. 기본적인 사용 방법은 `Dataset`을 정의하고, 이를 `DataLoader`에 전달하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커스텀 데이터셋을 만들 때, 일단 가장 기본적인 뼈대는 아래와 같다. 여기서 필수적으로 `overriding`해야 하는 메소드는 3개다.\n",
    "```\n",
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "  def __init__(self):\n",
    "  # 데이터셋의 전처리를 해주는 부분\n",
    "  \n",
    "  def __len__(self):\n",
    "  # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "  # len(dataset)을 했을 때 데이터셋의 크기를 리턴하는 magic method\n",
    "  \n",
    "  def __getitem__(self, idx): \n",
    "  # 데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "  # dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 magic method\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 들어 가기에 앞서: Change betwwen numpy.ndarray and torch.Tensor\n",
    "현재까지 딥러닝은 도메인에 적합한 여러 아키텍쳐가 나왔다. 하지만 데이터도 많고 모델이 큰 만큼 파라미터도 많기 때문에 최적화된 행렬(텐서) 연산을 사용한다. 따라서 데이터도 행렬처럼 표현이 가능해야 한다. 이때 주로 많이 사용하는 것이 `numpy` 라이브러리의 `ndarray` 클래스다. 한편, `torch`를 이용한 딥러닝에서 행렬 연산에 최적화되어 사용되는 실제 텐서는  `Tensor` 클래스다. 따라서 필요에 따라 둘 사이의 자유로운 변환을 할 줄 알아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy에서 Tensor로: torch.Tensor() vs torch.from_numpy()\n",
    "- torch.Tensor() 는 Numpy array의 사본일 뿐이다. 그래서 tensor의 값을 변경하더라도 Numpy array자체의 값이 달라지지 않는다. 하지만 torch.from_numpy()는 자동으로 input array의 dtype을 상속받고 tensor와 메모리 버퍼를 공유하기 때문에 tensor의 값이 변경되면 Numpy array값이 변경된다. 예시를 통해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) torch.Tensor()\n",
    "\n",
    "tensor로 변환할 때 새 메모리를 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor: tensor([1., 2., 3., 4.])\n",
      "Tensor is changed: tensor([-1.,  2.,  3.,  4.])\n",
      "But it cannot change np.array: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a=np.array([1,2,3,4])\n",
    "b=torch.Tensor(a)\n",
    "print('output tensor:',b)\n",
    "\n",
    "b[0]=-1\n",
    "print('Tensor is changed:', b)\n",
    "print('But it cannot change np.array:',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) torch.from_numpy()\n",
    "\n",
    "tensor로 변환할 때, 원래 메모리를 상속받는다. (=as_tensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output tensor: tensor([1, 2, 3, 4])\n",
      "Tensor is changed: tensor([-1,  2,  3,  4])\n",
      "It can change np.array: [-1  2  3  4]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=torch.from_numpy(a)\n",
    "print('output tensor:',b)\n",
    "\n",
    "b[0]=-1\n",
    "print('Tensor is changed:', b)\n",
    "print('It can change np.array:',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor에서 Numpy로: numpy()\n",
    "- 반대로 Tensor를 Numpy array로 바꾸고 싶다면 numpy()함수를 사용하면 된다. 사용법은 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8153333 , 0.53005856, 0.8592659 ],\n",
       "       [0.5831757 , 0.6514638 , 0.2412979 ],\n",
       "       [0.5439898 , 0.8240437 , 0.7072638 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "a = torch.rand(3,3)\n",
    "b = a.numpy()\n",
    "display(type(b))\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에 `torch.Tensor`와 `torch.tesnor()`의 차이, `torch.Tensor`와 `torch.autograd.Variable`의 차이, `Tensor`의 자료형, `Tensor`와 관련된 함수들 또는 메소드는 다음의 링크를 참고하라: https://subinium.github.io/pytorch-Tensor-Variable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Class Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "nHzO9sXUc60j"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "u3xgFyHBfrDy"
   },
   "outputs": [],
   "source": [
    "X_data = np.random.rand(10,10)\n",
    "y_data = np.ones(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make CustomDataset with Overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "0uVQHsNPdAQR"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, X_data, y_data):\n",
    "        # parent class 초기화. 범용성을 위해 python2 방식을 사용.\n",
    "        super(CustomDataset, self).__init__()\n",
    "        \n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    # 총 데이터의 개수를 리턴\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 PyTorch의 Tensor 형태로 리턴\n",
    "    def __getitem__(self, idx): \n",
    "        # y_data에서 인덱스를 이용해서 하나의 데이터만 뽑으면 scalar이다.\n",
    "        # 이를 Tensor라는 클래스 생성자에 바로 전달하니 __getitem__호출 시 다음과 같은 에러가 발생한다.\n",
    "        # TypeError: new(): data must be a sequence (got numpy.float64)\n",
    "        # Tensor의 argument로 scalar가 아닌 seqeunce를 달라는 것 같은데, \n",
    "        # Tensor(1) 이런 식으로 scalar로 Tensor 생성자에 전달하면 1이 아닌 다른 값을 내뱉는다.\n",
    "        # 어떤 값인지는 알아 봐야겠지만, 안전하게 torch.tensor()함수를 사용하는 게 좋은 것 같다.\n",
    "        return torch.tensor(self.X_data[idx]), torch.tensor(self.y_data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "8DxTC-Kqghjq"
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 데이터 X_data가 instance attribute인 X_data에 잘 들어 간 것을 확인할 수 있다.\n",
    "X_data == dataset.X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data == dataset.y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CustomDataset의 길이가 잘 출력되는 것을 확인할 수 있다.\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.31682092, 0.35788936, 0.04984044, 0.35053424, 0.69713628,\n",
       "        0.40714328, 0.92830629, 0.24683282, 0.67307919, 0.90673534]),\n",
       " 1.0)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 데이터의 첫번째 값들\n",
    "X_data[0], y_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3168, 0.3579, 0.0498, 0.3505, 0.6971, 0.4071, 0.9283, 0.2468, 0.6731,\n",
       "         0.9067], dtype=torch.float64),\n",
       " tensor(1., dtype=torch.float64))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터의 텐서 값. 소수점이 생략된 것 빼고 동일하다.\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to Iris classifier using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']], columns=iris['feature_names']+['target'])\n",
    "iris_df['target'] = iris_df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.datasets`의 `load_iris`함수를 이용하면 iris와 관련된 데이터들을 딕셔너리와 유사한 타입(실제로는 `sklearn.utils.Bunch`)으로 반환한다. 이때 `'data'`와 `'target'`은 `numpy.ndarry` type이므로 이를 이용해서 텐서로 만들어 간단한 분류 모델을 만들어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(iris['data']))\n",
    "print(type(iris['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "        target  \n",
       "0       setosa  \n",
       "1       setosa  \n",
       "2       setosa  \n",
       "3       setosa  \n",
       "4       setosa  \n",
       "..         ...  \n",
       "145  virginica  \n",
       "146  virginica  \n",
       "147  virginica  \n",
       "148  virginica  \n",
       "149  virginica  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        super(IrisDataset, self).__init__()\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        # model parameter가 float32이기때문에 input도 dtype을 통일해야한다.\n",
    "        self.X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "        self.y_tensor = torch.tensor(y_data) #.reshape(-1,1))\n",
    "        # one-hot encoding을 하니 multi-target이라고 되려 성질을 낸다. 과한 친절이었나 보다.\n",
    "#         self.y_tensor_one_hot = torch.nn.functional.one_hot(self.y_tensor, num_classes=3)\n",
    "#         self.y_tensor_one_hot = torch.as_tensor(self.y_tensor_one_hot, dtype=torch.bool)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_tensor[idx], self.y_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = IrisDataset(iris['data'], iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][0].dtype)\n",
    "print(dataset[0][1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
       "         [4.9000, 3.0000, 1.4000, 0.2000]]),\n",
       " tensor([0, 0]))"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(iris['data'][0], iris['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPclassifier(torch.nn.Module):\n",
    "    def __init__(self, num_input, H1, H2, num_classes):\n",
    "        super(MLPclassifier, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(num_input, H1)\n",
    "        self.linear2 = torch.nn.Linear(H1, H2)\n",
    "        self.linear3 = torch.nn.Linear(H2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.linear1(x))\n",
    "        x = torch.nn.functional.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "model = MLPclassifier(len(dataset[0][0]), 20, 10, 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100, Batch: 1/5, Loss: 1.15739\n",
      "Epoch: 0/100, Batch: 2/5, Loss: 1.15915\n",
      "Epoch: 0/100, Batch: 3/5, Loss: 1.14364\n",
      "Epoch: 0/100, Batch: 4/5, Loss: 1.08853\n",
      "Epoch: 0/100, Batch: 5/5, Loss: 1.15817\n",
      "Epoch: 1/100, Batch: 1/5, Loss: 1.13725\n",
      "Epoch: 1/100, Batch: 2/5, Loss: 1.1217\n",
      "Epoch: 1/100, Batch: 3/5, Loss: 1.06407\n",
      "Epoch: 1/100, Batch: 4/5, Loss: 1.12542\n",
      "Epoch: 1/100, Batch: 5/5, Loss: 1.15151\n",
      "Epoch: 2/100, Batch: 1/5, Loss: 1.13238\n",
      "Epoch: 2/100, Batch: 2/5, Loss: 1.07511\n",
      "Epoch: 2/100, Batch: 3/5, Loss: 1.08856\n",
      "Epoch: 2/100, Batch: 4/5, Loss: 1.10477\n",
      "Epoch: 2/100, Batch: 5/5, Loss: 1.10904\n",
      "Epoch: 3/100, Batch: 1/5, Loss: 1.09208\n",
      "Epoch: 3/100, Batch: 2/5, Loss: 1.08855\n",
      "Epoch: 3/100, Batch: 3/5, Loss: 1.08984\n",
      "Epoch: 3/100, Batch: 4/5, Loss: 1.09145\n",
      "Epoch: 3/100, Batch: 5/5, Loss: 1.06584\n",
      "Epoch: 4/100, Batch: 1/5, Loss: 1.07487\n",
      "Epoch: 4/100, Batch: 2/5, Loss: 1.0881\n",
      "Epoch: 4/100, Batch: 3/5, Loss: 1.06815\n",
      "Epoch: 4/100, Batch: 4/5, Loss: 1.06457\n",
      "Epoch: 4/100, Batch: 5/5, Loss: 1.0744\n",
      "Epoch: 5/100, Batch: 1/5, Loss: 1.06788\n",
      "Epoch: 5/100, Batch: 2/5, Loss: 1.06854\n",
      "Epoch: 5/100, Batch: 3/5, Loss: 1.05339\n",
      "Epoch: 5/100, Batch: 4/5, Loss: 1.06461\n",
      "Epoch: 5/100, Batch: 5/5, Loss: 1.05364\n",
      "Epoch: 6/100, Batch: 1/5, Loss: 1.0555\n",
      "Epoch: 6/100, Batch: 2/5, Loss: 1.06326\n",
      "Epoch: 6/100, Batch: 3/5, Loss: 1.05278\n",
      "Epoch: 6/100, Batch: 4/5, Loss: 1.0561\n",
      "Epoch: 6/100, Batch: 5/5, Loss: 1.02004\n",
      "Epoch: 7/100, Batch: 1/5, Loss: 1.03988\n",
      "Epoch: 7/100, Batch: 2/5, Loss: 1.0488\n",
      "Epoch: 7/100, Batch: 3/5, Loss: 1.0442\n",
      "Epoch: 7/100, Batch: 4/5, Loss: 1.03967\n",
      "Epoch: 7/100, Batch: 5/5, Loss: 1.03784\n",
      "Epoch: 8/100, Batch: 1/5, Loss: 1.04122\n",
      "Epoch: 8/100, Batch: 2/5, Loss: 1.02128\n",
      "Epoch: 8/100, Batch: 3/5, Loss: 1.03504\n",
      "Epoch: 8/100, Batch: 4/5, Loss: 1.04488\n",
      "Epoch: 8/100, Batch: 5/5, Loss: 1.01448\n",
      "Epoch: 9/100, Batch: 1/5, Loss: 1.02756\n",
      "Epoch: 9/100, Batch: 2/5, Loss: 1.01325\n",
      "Epoch: 9/100, Batch: 3/5, Loss: 1.0279\n",
      "Epoch: 9/100, Batch: 4/5, Loss: 1.0174\n",
      "Epoch: 9/100, Batch: 5/5, Loss: 1.02836\n",
      "Epoch: 10/100, Batch: 1/5, Loss: 1.02803\n",
      "Epoch: 10/100, Batch: 2/5, Loss: 0.99951\n",
      "Epoch: 10/100, Batch: 3/5, Loss: 1.0232\n",
      "Epoch: 10/100, Batch: 4/5, Loss: 0.99341\n",
      "Epoch: 10/100, Batch: 5/5, Loss: 1.01784\n",
      "Epoch: 11/100, Batch: 1/5, Loss: 0.99402\n",
      "Epoch: 11/100, Batch: 2/5, Loss: 1.02913\n",
      "Epoch: 11/100, Batch: 3/5, Loss: 1.00226\n",
      "Epoch: 11/100, Batch: 4/5, Loss: 0.99999\n",
      "Epoch: 11/100, Batch: 5/5, Loss: 0.97086\n",
      "Epoch: 12/100, Batch: 1/5, Loss: 0.9779\n",
      "Epoch: 12/100, Batch: 2/5, Loss: 0.99509\n",
      "Epoch: 12/100, Batch: 3/5, Loss: 1.01152\n",
      "Epoch: 12/100, Batch: 4/5, Loss: 0.95463\n",
      "Epoch: 12/100, Batch: 5/5, Loss: 1.01944\n",
      "Epoch: 13/100, Batch: 1/5, Loss: 0.96931\n",
      "Epoch: 13/100, Batch: 2/5, Loss: 0.96622\n",
      "Epoch: 13/100, Batch: 3/5, Loss: 0.99508\n",
      "Epoch: 13/100, Batch: 4/5, Loss: 0.97223\n",
      "Epoch: 13/100, Batch: 5/5, Loss: 0.98739\n",
      "Epoch: 14/100, Batch: 1/5, Loss: 0.99421\n",
      "Epoch: 14/100, Batch: 2/5, Loss: 0.95268\n",
      "Epoch: 14/100, Batch: 3/5, Loss: 0.98215\n",
      "Epoch: 14/100, Batch: 4/5, Loss: 0.93153\n",
      "Epoch: 14/100, Batch: 5/5, Loss: 0.95958\n",
      "Epoch: 15/100, Batch: 1/5, Loss: 0.96687\n",
      "Epoch: 15/100, Batch: 2/5, Loss: 0.96206\n",
      "Epoch: 15/100, Batch: 3/5, Loss: 0.92404\n",
      "Epoch: 15/100, Batch: 4/5, Loss: 0.97149\n",
      "Epoch: 15/100, Batch: 5/5, Loss: 0.91797\n",
      "Epoch: 16/100, Batch: 1/5, Loss: 0.92172\n",
      "Epoch: 16/100, Batch: 2/5, Loss: 0.94536\n",
      "Epoch: 16/100, Batch: 3/5, Loss: 0.95297\n",
      "Epoch: 16/100, Batch: 4/5, Loss: 0.93309\n",
      "Epoch: 16/100, Batch: 5/5, Loss: 0.91608\n",
      "Epoch: 17/100, Batch: 1/5, Loss: 0.90971\n",
      "Epoch: 17/100, Batch: 2/5, Loss: 0.904\n",
      "Epoch: 17/100, Batch: 3/5, Loss: 0.9593\n",
      "Epoch: 17/100, Batch: 4/5, Loss: 0.93131\n",
      "Epoch: 17/100, Batch: 5/5, Loss: 0.87428\n",
      "Epoch: 18/100, Batch: 1/5, Loss: 0.86317\n",
      "Epoch: 18/100, Batch: 2/5, Loss: 0.95197\n",
      "Epoch: 18/100, Batch: 3/5, Loss: 0.88665\n",
      "Epoch: 18/100, Batch: 4/5, Loss: 0.88566\n",
      "Epoch: 18/100, Batch: 5/5, Loss: 0.92671\n",
      "Epoch: 19/100, Batch: 1/5, Loss: 0.86031\n",
      "Epoch: 19/100, Batch: 2/5, Loss: 0.89662\n",
      "Epoch: 19/100, Batch: 3/5, Loss: 0.87849\n",
      "Epoch: 19/100, Batch: 4/5, Loss: 0.90898\n",
      "Epoch: 19/100, Batch: 5/5, Loss: 0.85183\n",
      "Epoch: 20/100, Batch: 1/5, Loss: 0.8852\n",
      "Epoch: 20/100, Batch: 2/5, Loss: 0.83753\n",
      "Epoch: 20/100, Batch: 3/5, Loss: 0.85837\n",
      "Epoch: 20/100, Batch: 4/5, Loss: 0.86812\n",
      "Epoch: 20/100, Batch: 5/5, Loss: 0.85278\n",
      "Epoch: 21/100, Batch: 1/5, Loss: 0.82617\n",
      "Epoch: 21/100, Batch: 2/5, Loss: 0.84352\n",
      "Epoch: 21/100, Batch: 3/5, Loss: 0.84107\n",
      "Epoch: 21/100, Batch: 4/5, Loss: 0.86308\n",
      "Epoch: 21/100, Batch: 5/5, Loss: 0.80116\n",
      "Epoch: 22/100, Batch: 1/5, Loss: 0.7799\n",
      "Epoch: 22/100, Batch: 2/5, Loss: 0.82362\n",
      "Epoch: 22/100, Batch: 3/5, Loss: 0.81831\n",
      "Epoch: 22/100, Batch: 4/5, Loss: 0.87158\n",
      "Epoch: 22/100, Batch: 5/5, Loss: 0.75493\n",
      "Epoch: 23/100, Batch: 1/5, Loss: 0.80215\n",
      "Epoch: 23/100, Batch: 2/5, Loss: 0.76953\n",
      "Epoch: 23/100, Batch: 3/5, Loss: 0.78306\n",
      "Epoch: 23/100, Batch: 4/5, Loss: 0.84583\n",
      "Epoch: 23/100, Batch: 5/5, Loss: 0.71919\n",
      "Epoch: 24/100, Batch: 1/5, Loss: 0.74238\n",
      "Epoch: 24/100, Batch: 2/5, Loss: 0.8399\n",
      "Epoch: 24/100, Batch: 3/5, Loss: 0.74009\n",
      "Epoch: 24/100, Batch: 4/5, Loss: 0.76294\n",
      "Epoch: 24/100, Batch: 5/5, Loss: 0.70899\n",
      "Epoch: 25/100, Batch: 1/5, Loss: 0.75607\n",
      "Epoch: 25/100, Batch: 2/5, Loss: 0.77461\n",
      "Epoch: 25/100, Batch: 3/5, Loss: 0.69573\n",
      "Epoch: 25/100, Batch: 4/5, Loss: 0.72459\n",
      "Epoch: 25/100, Batch: 5/5, Loss: 0.72871\n",
      "Epoch: 26/100, Batch: 1/5, Loss: 0.71606\n",
      "Epoch: 26/100, Batch: 2/5, Loss: 0.76581\n",
      "Epoch: 26/100, Batch: 3/5, Loss: 0.67685\n",
      "Epoch: 26/100, Batch: 4/5, Loss: 0.68562\n",
      "Epoch: 26/100, Batch: 5/5, Loss: 0.70916\n",
      "Epoch: 27/100, Batch: 1/5, Loss: 0.68821\n",
      "Epoch: 27/100, Batch: 2/5, Loss: 0.70403\n",
      "Epoch: 27/100, Batch: 3/5, Loss: 0.6807\n",
      "Epoch: 27/100, Batch: 4/5, Loss: 0.64313\n",
      "Epoch: 27/100, Batch: 5/5, Loss: 0.71952\n",
      "Epoch: 28/100, Batch: 1/5, Loss: 0.6272\n",
      "Epoch: 28/100, Batch: 2/5, Loss: 0.63668\n",
      "Epoch: 28/100, Batch: 3/5, Loss: 0.7093\n",
      "Epoch: 28/100, Batch: 4/5, Loss: 0.62534\n",
      "Epoch: 28/100, Batch: 5/5, Loss: 0.71246\n",
      "Epoch: 29/100, Batch: 1/5, Loss: 0.63598\n",
      "Epoch: 29/100, Batch: 2/5, Loss: 0.66743\n",
      "Epoch: 29/100, Batch: 3/5, Loss: 0.60619\n",
      "Epoch: 29/100, Batch: 4/5, Loss: 0.58584\n",
      "Epoch: 29/100, Batch: 5/5, Loss: 0.68011\n",
      "Epoch: 30/100, Batch: 1/5, Loss: 0.55385\n",
      "Epoch: 30/100, Batch: 2/5, Loss: 0.63873\n",
      "Epoch: 30/100, Batch: 3/5, Loss: 0.5947\n",
      "Epoch: 30/100, Batch: 4/5, Loss: 0.6437\n",
      "Epoch: 30/100, Batch: 5/5, Loss: 0.59636\n",
      "Epoch: 31/100, Batch: 1/5, Loss: 0.57996\n",
      "Epoch: 31/100, Batch: 2/5, Loss: 0.60434\n",
      "Epoch: 31/100, Batch: 3/5, Loss: 0.5646\n",
      "Epoch: 31/100, Batch: 4/5, Loss: 0.57437\n",
      "Epoch: 31/100, Batch: 5/5, Loss: 0.58094\n",
      "Epoch: 32/100, Batch: 1/5, Loss: 0.58012\n",
      "Epoch: 32/100, Batch: 2/5, Loss: 0.5668\n",
      "Epoch: 32/100, Batch: 3/5, Loss: 0.57801\n",
      "Epoch: 32/100, Batch: 4/5, Loss: 0.53215\n",
      "Epoch: 32/100, Batch: 5/5, Loss: 0.51321\n",
      "Epoch: 33/100, Batch: 1/5, Loss: 0.50163\n",
      "Epoch: 33/100, Batch: 2/5, Loss: 0.58354\n",
      "Epoch: 33/100, Batch: 3/5, Loss: 0.53431\n",
      "Epoch: 33/100, Batch: 4/5, Loss: 0.50763\n",
      "Epoch: 33/100, Batch: 5/5, Loss: 0.54126\n",
      "Epoch: 34/100, Batch: 1/5, Loss: 0.57965\n",
      "Epoch: 34/100, Batch: 2/5, Loss: 0.46937\n",
      "Epoch: 34/100, Batch: 3/5, Loss: 0.51327\n",
      "Epoch: 34/100, Batch: 4/5, Loss: 0.5177\n",
      "Epoch: 34/100, Batch: 5/5, Loss: 0.46267\n",
      "Epoch: 35/100, Batch: 1/5, Loss: 0.45022\n",
      "Epoch: 35/100, Batch: 2/5, Loss: 0.42445\n",
      "Epoch: 35/100, Batch: 3/5, Loss: 0.53376\n",
      "Epoch: 35/100, Batch: 4/5, Loss: 0.54622\n",
      "Epoch: 35/100, Batch: 5/5, Loss: 0.50536\n",
      "Epoch: 36/100, Batch: 1/5, Loss: 0.44554\n",
      "Epoch: 36/100, Batch: 2/5, Loss: 0.42968\n",
      "Epoch: 36/100, Batch: 3/5, Loss: 0.42922\n",
      "Epoch: 36/100, Batch: 4/5, Loss: 0.47403\n",
      "Epoch: 36/100, Batch: 5/5, Loss: 0.63172\n",
      "Epoch: 37/100, Batch: 1/5, Loss: 0.42394\n",
      "Epoch: 37/100, Batch: 2/5, Loss: 0.50097\n",
      "Epoch: 37/100, Batch: 3/5, Loss: 0.49436\n",
      "Epoch: 37/100, Batch: 4/5, Loss: 0.42174\n",
      "Epoch: 37/100, Batch: 5/5, Loss: 0.42086\n",
      "Epoch: 38/100, Batch: 1/5, Loss: 0.38399\n",
      "Epoch: 38/100, Batch: 2/5, Loss: 0.45169\n",
      "Epoch: 38/100, Batch: 3/5, Loss: 0.42864\n",
      "Epoch: 38/100, Batch: 4/5, Loss: 0.49275\n",
      "Epoch: 38/100, Batch: 5/5, Loss: 0.43026\n",
      "Epoch: 39/100, Batch: 1/5, Loss: 0.44368\n",
      "Epoch: 39/100, Batch: 2/5, Loss: 0.47697\n",
      "Epoch: 39/100, Batch: 3/5, Loss: 0.34857\n",
      "Epoch: 39/100, Batch: 4/5, Loss: 0.41024\n",
      "Epoch: 39/100, Batch: 5/5, Loss: 0.44075\n",
      "Epoch: 40/100, Batch: 1/5, Loss: 0.43618\n",
      "Epoch: 40/100, Batch: 2/5, Loss: 0.40103\n",
      "Epoch: 40/100, Batch: 3/5, Loss: 0.41968\n",
      "Epoch: 40/100, Batch: 4/5, Loss: 0.39117\n",
      "Epoch: 40/100, Batch: 5/5, Loss: 0.38829\n",
      "Epoch: 41/100, Batch: 1/5, Loss: 0.39611\n",
      "Epoch: 41/100, Batch: 2/5, Loss: 0.43028\n",
      "Epoch: 41/100, Batch: 3/5, Loss: 0.42391\n",
      "Epoch: 41/100, Batch: 4/5, Loss: 0.34785\n",
      "Epoch: 41/100, Batch: 5/5, Loss: 0.36634\n",
      "Epoch: 42/100, Batch: 1/5, Loss: 0.33747\n",
      "Epoch: 42/100, Batch: 2/5, Loss: 0.37879\n",
      "Epoch: 42/100, Batch: 3/5, Loss: 0.41409\n",
      "Epoch: 42/100, Batch: 4/5, Loss: 0.39359\n",
      "Epoch: 42/100, Batch: 5/5, Loss: 0.38797\n",
      "Epoch: 43/100, Batch: 1/5, Loss: 0.35224\n",
      "Epoch: 43/100, Batch: 2/5, Loss: 0.42446\n",
      "Epoch: 43/100, Batch: 3/5, Loss: 0.41693\n",
      "Epoch: 43/100, Batch: 4/5, Loss: 0.33023\n",
      "Epoch: 43/100, Batch: 5/5, Loss: 0.30624\n",
      "Epoch: 44/100, Batch: 1/5, Loss: 0.38769\n",
      "Epoch: 44/100, Batch: 2/5, Loss: 0.40346\n",
      "Epoch: 44/100, Batch: 3/5, Loss: 0.36881\n",
      "Epoch: 44/100, Batch: 4/5, Loss: 0.32635\n",
      "Epoch: 44/100, Batch: 5/5, Loss: 0.28524\n",
      "Epoch: 45/100, Batch: 1/5, Loss: 0.31944\n",
      "Epoch: 45/100, Batch: 2/5, Loss: 0.3717\n",
      "Epoch: 45/100, Batch: 3/5, Loss: 0.34471\n",
      "Epoch: 45/100, Batch: 4/5, Loss: 0.34315\n",
      "Epoch: 45/100, Batch: 5/5, Loss: 0.36416\n",
      "Epoch: 46/100, Batch: 1/5, Loss: 0.29449\n",
      "Epoch: 46/100, Batch: 2/5, Loss: 0.37569\n",
      "Epoch: 46/100, Batch: 3/5, Loss: 0.37135\n",
      "Epoch: 46/100, Batch: 4/5, Loss: 0.2852\n",
      "Epoch: 46/100, Batch: 5/5, Loss: 0.36909\n",
      "Epoch: 47/100, Batch: 1/5, Loss: 0.35098\n",
      "Epoch: 47/100, Batch: 2/5, Loss: 0.30579\n",
      "Epoch: 47/100, Batch: 3/5, Loss: 0.31728\n",
      "Epoch: 47/100, Batch: 4/5, Loss: 0.32356\n",
      "Epoch: 47/100, Batch: 5/5, Loss: 0.34905\n",
      "Epoch: 48/100, Batch: 1/5, Loss: 0.3027\n",
      "Epoch: 48/100, Batch: 2/5, Loss: 0.36398\n",
      "Epoch: 48/100, Batch: 3/5, Loss: 0.29963\n",
      "Epoch: 48/100, Batch: 4/5, Loss: 0.3251\n",
      "Epoch: 48/100, Batch: 5/5, Loss: 0.2875\n",
      "Epoch: 49/100, Batch: 1/5, Loss: 0.25937\n",
      "Epoch: 49/100, Batch: 2/5, Loss: 0.35494\n",
      "Epoch: 49/100, Batch: 3/5, Loss: 0.34036\n",
      "Epoch: 49/100, Batch: 4/5, Loss: 0.27871\n",
      "Epoch: 49/100, Batch: 5/5, Loss: 0.30985\n",
      "Epoch: 50/100, Batch: 1/5, Loss: 0.2718\n",
      "Epoch: 50/100, Batch: 2/5, Loss: 0.3137\n",
      "Epoch: 50/100, Batch: 3/5, Loss: 0.2654\n",
      "Epoch: 50/100, Batch: 4/5, Loss: 0.34925\n",
      "Epoch: 50/100, Batch: 5/5, Loss: 0.29797\n",
      "Epoch: 51/100, Batch: 1/5, Loss: 0.27951\n",
      "Epoch: 51/100, Batch: 2/5, Loss: 0.29072\n",
      "Epoch: 51/100, Batch: 3/5, Loss: 0.29486\n",
      "Epoch: 51/100, Batch: 4/5, Loss: 0.32331\n",
      "Epoch: 51/100, Batch: 5/5, Loss: 0.25894\n",
      "Epoch: 52/100, Batch: 1/5, Loss: 0.3113\n",
      "Epoch: 52/100, Batch: 2/5, Loss: 0.36331\n",
      "Epoch: 52/100, Batch: 3/5, Loss: 0.26455\n",
      "Epoch: 52/100, Batch: 4/5, Loss: 0.25813\n",
      "Epoch: 52/100, Batch: 5/5, Loss: 0.19678\n",
      "Epoch: 53/100, Batch: 1/5, Loss: 0.29243\n",
      "Epoch: 53/100, Batch: 2/5, Loss: 0.31397\n",
      "Epoch: 53/100, Batch: 3/5, Loss: 0.20138\n",
      "Epoch: 53/100, Batch: 4/5, Loss: 0.22891\n",
      "Epoch: 53/100, Batch: 5/5, Loss: 0.37076\n",
      "Epoch: 54/100, Batch: 1/5, Loss: 0.25461\n",
      "Epoch: 54/100, Batch: 2/5, Loss: 0.2773\n",
      "Epoch: 54/100, Batch: 3/5, Loss: 0.26402\n",
      "Epoch: 54/100, Batch: 4/5, Loss: 0.29939\n",
      "Epoch: 54/100, Batch: 5/5, Loss: 0.23438\n",
      "Epoch: 55/100, Batch: 1/5, Loss: 0.30227\n",
      "Epoch: 55/100, Batch: 2/5, Loss: 0.24144\n",
      "Epoch: 55/100, Batch: 3/5, Loss: 0.30069\n",
      "Epoch: 55/100, Batch: 4/5, Loss: 0.2141\n",
      "Epoch: 55/100, Batch: 5/5, Loss: 0.23847\n",
      "Epoch: 56/100, Batch: 1/5, Loss: 0.22436\n",
      "Epoch: 56/100, Batch: 2/5, Loss: 0.19904\n",
      "Epoch: 56/100, Batch: 3/5, Loss: 0.30269\n",
      "Epoch: 56/100, Batch: 4/5, Loss: 0.33311\n",
      "Epoch: 56/100, Batch: 5/5, Loss: 0.19316\n",
      "Epoch: 57/100, Batch: 1/5, Loss: 0.23224\n",
      "Epoch: 57/100, Batch: 2/5, Loss: 0.24604\n",
      "Epoch: 57/100, Batch: 3/5, Loss: 0.23514\n",
      "Epoch: 57/100, Batch: 4/5, Loss: 0.26954\n",
      "Epoch: 57/100, Batch: 5/5, Loss: 0.25565\n",
      "Epoch: 58/100, Batch: 1/5, Loss: 0.19798\n",
      "Epoch: 58/100, Batch: 2/5, Loss: 0.2441\n",
      "Epoch: 58/100, Batch: 3/5, Loss: 0.29963\n",
      "Epoch: 58/100, Batch: 4/5, Loss: 0.17734\n",
      "Epoch: 58/100, Batch: 5/5, Loss: 0.30588\n",
      "Epoch: 59/100, Batch: 1/5, Loss: 0.22518\n",
      "Epoch: 59/100, Batch: 2/5, Loss: 0.18865\n",
      "Epoch: 59/100, Batch: 3/5, Loss: 0.24115\n",
      "Epoch: 59/100, Batch: 4/5, Loss: 0.279\n",
      "Epoch: 59/100, Batch: 5/5, Loss: 0.24903\n",
      "Epoch: 60/100, Batch: 1/5, Loss: 0.20758\n",
      "Epoch: 60/100, Batch: 2/5, Loss: 0.19805\n",
      "Epoch: 60/100, Batch: 3/5, Loss: 0.24745\n",
      "Epoch: 60/100, Batch: 4/5, Loss: 0.20907\n",
      "Epoch: 60/100, Batch: 5/5, Loss: 0.3132\n",
      "Epoch: 61/100, Batch: 1/5, Loss: 0.22205\n",
      "Epoch: 61/100, Batch: 2/5, Loss: 0.20102\n",
      "Epoch: 61/100, Batch: 3/5, Loss: 0.26432\n",
      "Epoch: 61/100, Batch: 4/5, Loss: 0.2129\n",
      "Epoch: 61/100, Batch: 5/5, Loss: 0.21514\n",
      "Epoch: 62/100, Batch: 1/5, Loss: 0.2061\n",
      "Epoch: 62/100, Batch: 2/5, Loss: 0.19364\n",
      "Epoch: 62/100, Batch: 3/5, Loss: 0.21733\n",
      "Epoch: 62/100, Batch: 4/5, Loss: 0.2476\n",
      "Epoch: 62/100, Batch: 5/5, Loss: 0.22996\n",
      "Epoch: 63/100, Batch: 1/5, Loss: 0.1854\n",
      "Epoch: 63/100, Batch: 2/5, Loss: 0.24075\n",
      "Epoch: 63/100, Batch: 3/5, Loss: 0.22136\n",
      "Epoch: 63/100, Batch: 4/5, Loss: 0.21745\n",
      "Epoch: 63/100, Batch: 5/5, Loss: 0.19254\n",
      "Epoch: 64/100, Batch: 1/5, Loss: 0.2427\n",
      "Epoch: 64/100, Batch: 2/5, Loss: 0.22411\n",
      "Epoch: 64/100, Batch: 3/5, Loss: 0.19664\n",
      "Epoch: 64/100, Batch: 4/5, Loss: 0.15435\n",
      "Epoch: 64/100, Batch: 5/5, Loss: 0.22908\n",
      "Epoch: 65/100, Batch: 1/5, Loss: 0.19011\n",
      "Epoch: 65/100, Batch: 2/5, Loss: 0.18873\n",
      "Epoch: 65/100, Batch: 3/5, Loss: 0.16748\n",
      "Epoch: 65/100, Batch: 4/5, Loss: 0.25035\n",
      "Epoch: 65/100, Batch: 5/5, Loss: 0.22517\n",
      "Epoch: 66/100, Batch: 1/5, Loss: 0.18847\n",
      "Epoch: 66/100, Batch: 2/5, Loss: 0.14183\n",
      "Epoch: 66/100, Batch: 3/5, Loss: 0.18143\n",
      "Epoch: 66/100, Batch: 4/5, Loss: 0.20076\n",
      "Epoch: 66/100, Batch: 5/5, Loss: 0.32663\n",
      "Epoch: 67/100, Batch: 1/5, Loss: 0.14102\n",
      "Epoch: 67/100, Batch: 2/5, Loss: 0.23592\n",
      "Epoch: 67/100, Batch: 3/5, Loss: 0.19014\n",
      "Epoch: 67/100, Batch: 4/5, Loss: 0.24499\n",
      "Epoch: 67/100, Batch: 5/5, Loss: 0.14236\n",
      "Epoch: 68/100, Batch: 1/5, Loss: 0.16846\n",
      "Epoch: 68/100, Batch: 2/5, Loss: 0.22245\n",
      "Epoch: 68/100, Batch: 3/5, Loss: 0.17025\n",
      "Epoch: 68/100, Batch: 4/5, Loss: 0.20458\n",
      "Epoch: 68/100, Batch: 5/5, Loss: 0.18686\n",
      "Epoch: 69/100, Batch: 1/5, Loss: 0.18847\n",
      "Epoch: 69/100, Batch: 2/5, Loss: 0.14316\n",
      "Epoch: 69/100, Batch: 3/5, Loss: 0.20819\n",
      "Epoch: 69/100, Batch: 4/5, Loss: 0.15376\n",
      "Epoch: 69/100, Batch: 5/5, Loss: 0.25871\n",
      "Epoch: 70/100, Batch: 1/5, Loss: 0.16386\n",
      "Epoch: 70/100, Batch: 2/5, Loss: 0.1894\n",
      "Epoch: 70/100, Batch: 3/5, Loss: 0.24382\n",
      "Epoch: 70/100, Batch: 4/5, Loss: 0.12524\n",
      "Epoch: 70/100, Batch: 5/5, Loss: 0.18537\n",
      "Epoch: 71/100, Batch: 1/5, Loss: 0.16051\n",
      "Epoch: 71/100, Batch: 2/5, Loss: 0.21378\n",
      "Epoch: 71/100, Batch: 3/5, Loss: 0.15019\n",
      "Epoch: 71/100, Batch: 4/5, Loss: 0.17659\n",
      "Epoch: 71/100, Batch: 5/5, Loss: 0.19732\n",
      "Epoch: 72/100, Batch: 1/5, Loss: 0.22299\n",
      "Epoch: 72/100, Batch: 2/5, Loss: 0.17293\n",
      "Epoch: 72/100, Batch: 3/5, Loss: 0.20105\n",
      "Epoch: 72/100, Batch: 4/5, Loss: 0.11862\n",
      "Epoch: 72/100, Batch: 5/5, Loss: 0.1469\n",
      "Epoch: 73/100, Batch: 1/5, Loss: 0.19984\n",
      "Epoch: 73/100, Batch: 2/5, Loss: 0.13805\n",
      "Epoch: 73/100, Batch: 3/5, Loss: 0.21667\n",
      "Epoch: 73/100, Batch: 4/5, Loss: 0.10459\n",
      "Epoch: 73/100, Batch: 5/5, Loss: 0.20643\n",
      "Epoch: 74/100, Batch: 1/5, Loss: 0.10829\n",
      "Epoch: 74/100, Batch: 2/5, Loss: 0.23967\n",
      "Epoch: 74/100, Batch: 3/5, Loss: 0.21356\n",
      "Epoch: 74/100, Batch: 4/5, Loss: 0.12851\n",
      "Epoch: 74/100, Batch: 5/5, Loss: 0.14842\n",
      "Epoch: 75/100, Batch: 1/5, Loss: 0.18051\n",
      "Epoch: 75/100, Batch: 2/5, Loss: 0.18162\n",
      "Epoch: 75/100, Batch: 3/5, Loss: 0.16845\n",
      "Epoch: 75/100, Batch: 4/5, Loss: 0.13584\n",
      "Epoch: 75/100, Batch: 5/5, Loss: 0.15305\n",
      "Epoch: 76/100, Batch: 1/5, Loss: 0.10703\n",
      "Epoch: 76/100, Batch: 2/5, Loss: 0.19159\n",
      "Epoch: 76/100, Batch: 3/5, Loss: 0.18581\n",
      "Epoch: 76/100, Batch: 4/5, Loss: 0.17577\n",
      "Epoch: 76/100, Batch: 5/5, Loss: 0.14282\n",
      "Epoch: 77/100, Batch: 1/5, Loss: 0.14736\n",
      "Epoch: 77/100, Batch: 2/5, Loss: 0.17735\n",
      "Epoch: 77/100, Batch: 3/5, Loss: 0.16601\n",
      "Epoch: 77/100, Batch: 4/5, Loss: 0.16858\n",
      "Epoch: 77/100, Batch: 5/5, Loss: 0.1224\n",
      "Epoch: 78/100, Batch: 1/5, Loss: 0.09361\n",
      "Epoch: 78/100, Batch: 2/5, Loss: 0.17036\n",
      "Epoch: 78/100, Batch: 3/5, Loss: 0.13809\n",
      "Epoch: 78/100, Batch: 4/5, Loss: 0.20444\n",
      "Epoch: 78/100, Batch: 5/5, Loss: 0.1755\n",
      "Epoch: 79/100, Batch: 1/5, Loss: 0.23801\n",
      "Epoch: 79/100, Batch: 2/5, Loss: 0.11828\n",
      "Epoch: 79/100, Batch: 3/5, Loss: 0.14524\n",
      "Epoch: 79/100, Batch: 4/5, Loss: 0.15562\n",
      "Epoch: 79/100, Batch: 5/5, Loss: 0.08603\n",
      "Epoch: 80/100, Batch: 1/5, Loss: 0.11614\n",
      "Epoch: 80/100, Batch: 2/5, Loss: 0.12422\n",
      "Epoch: 80/100, Batch: 3/5, Loss: 0.2089\n",
      "Epoch: 80/100, Batch: 4/5, Loss: 0.15167\n",
      "Epoch: 80/100, Batch: 5/5, Loss: 0.148\n",
      "Epoch: 81/100, Batch: 1/5, Loss: 0.12552\n",
      "Epoch: 81/100, Batch: 2/5, Loss: 0.15335\n",
      "Epoch: 81/100, Batch: 3/5, Loss: 0.17703\n",
      "Epoch: 81/100, Batch: 4/5, Loss: 0.13772\n",
      "Epoch: 81/100, Batch: 5/5, Loss: 0.14136\n",
      "Epoch: 82/100, Batch: 1/5, Loss: 0.1591\n",
      "Epoch: 82/100, Batch: 2/5, Loss: 0.12201\n",
      "Epoch: 82/100, Batch: 3/5, Loss: 0.09712\n",
      "Epoch: 82/100, Batch: 4/5, Loss: 0.18257\n",
      "Epoch: 82/100, Batch: 5/5, Loss: 0.17152\n",
      "Epoch: 83/100, Batch: 1/5, Loss: 0.14594\n",
      "Epoch: 83/100, Batch: 2/5, Loss: 0.13996\n",
      "Epoch: 83/100, Batch: 3/5, Loss: 0.07532\n",
      "Epoch: 83/100, Batch: 4/5, Loss: 0.16794\n",
      "Epoch: 83/100, Batch: 5/5, Loss: 0.20057\n",
      "Epoch: 84/100, Batch: 1/5, Loss: 0.1585\n",
      "Epoch: 84/100, Batch: 2/5, Loss: 0.10834\n",
      "Epoch: 84/100, Batch: 3/5, Loss: 0.13993\n",
      "Epoch: 84/100, Batch: 4/5, Loss: 0.1383\n",
      "Epoch: 84/100, Batch: 5/5, Loss: 0.16288\n",
      "Epoch: 85/100, Batch: 1/5, Loss: 0.12276\n",
      "Epoch: 85/100, Batch: 2/5, Loss: 0.15345\n",
      "Epoch: 85/100, Batch: 3/5, Loss: 0.12296\n",
      "Epoch: 85/100, Batch: 4/5, Loss: 0.13851\n",
      "Epoch: 85/100, Batch: 5/5, Loss: 0.15936\n",
      "Epoch: 86/100, Batch: 1/5, Loss: 0.12512\n",
      "Epoch: 86/100, Batch: 2/5, Loss: 0.10435\n",
      "Epoch: 86/100, Batch: 3/5, Loss: 0.13038\n",
      "Epoch: 86/100, Batch: 4/5, Loss: 0.17483\n",
      "Epoch: 86/100, Batch: 5/5, Loss: 0.15063\n",
      "Epoch: 87/100, Batch: 1/5, Loss: 0.10086\n",
      "Epoch: 87/100, Batch: 2/5, Loss: 0.21344\n",
      "Epoch: 87/100, Batch: 3/5, Loss: 0.13761\n",
      "Epoch: 87/100, Batch: 4/5, Loss: 0.10348\n",
      "Epoch: 87/100, Batch: 5/5, Loss: 0.10489\n",
      "Epoch: 88/100, Batch: 1/5, Loss: 0.19234\n",
      "Epoch: 88/100, Batch: 2/5, Loss: 0.11975\n",
      "Epoch: 88/100, Batch: 3/5, Loss: 0.12039\n",
      "Epoch: 88/100, Batch: 4/5, Loss: 0.098\n",
      "Epoch: 88/100, Batch: 5/5, Loss: 0.12597\n",
      "Epoch: 89/100, Batch: 1/5, Loss: 0.11128\n",
      "Epoch: 89/100, Batch: 2/5, Loss: 0.16781\n",
      "Epoch: 89/100, Batch: 3/5, Loss: 0.16228\n",
      "Epoch: 89/100, Batch: 4/5, Loss: 0.12292\n",
      "Epoch: 89/100, Batch: 5/5, Loss: 0.06258\n",
      "Epoch: 90/100, Batch: 1/5, Loss: 0.18639\n",
      "Epoch: 90/100, Batch: 2/5, Loss: 0.08936\n",
      "Epoch: 90/100, Batch: 3/5, Loss: 0.08634\n",
      "Epoch: 90/100, Batch: 4/5, Loss: 0.16982\n",
      "Epoch: 90/100, Batch: 5/5, Loss: 0.09781\n",
      "Epoch: 91/100, Batch: 1/5, Loss: 0.16986\n",
      "Epoch: 91/100, Batch: 2/5, Loss: 0.06627\n",
      "Epoch: 91/100, Batch: 3/5, Loss: 0.17843\n",
      "Epoch: 91/100, Batch: 4/5, Loss: 0.10305\n",
      "Epoch: 91/100, Batch: 5/5, Loss: 0.11141\n",
      "Epoch: 92/100, Batch: 1/5, Loss: 0.13729\n",
      "Epoch: 92/100, Batch: 2/5, Loss: 0.07115\n",
      "Epoch: 92/100, Batch: 3/5, Loss: 0.08857\n",
      "Epoch: 92/100, Batch: 4/5, Loss: 0.16141\n",
      "Epoch: 92/100, Batch: 5/5, Loss: 0.182\n",
      "Epoch: 93/100, Batch: 1/5, Loss: 0.1085\n",
      "Epoch: 93/100, Batch: 2/5, Loss: 0.09583\n",
      "Epoch: 93/100, Batch: 3/5, Loss: 0.12341\n",
      "Epoch: 93/100, Batch: 4/5, Loss: 0.13427\n",
      "Epoch: 93/100, Batch: 5/5, Loss: 0.16321\n",
      "Epoch: 94/100, Batch: 1/5, Loss: 0.12461\n",
      "Epoch: 94/100, Batch: 2/5, Loss: 0.1525\n",
      "Epoch: 94/100, Batch: 3/5, Loss: 0.09038\n",
      "Epoch: 94/100, Batch: 4/5, Loss: 0.09853\n",
      "Epoch: 94/100, Batch: 5/5, Loss: 0.14919\n",
      "Epoch: 95/100, Batch: 1/5, Loss: 0.13285\n",
      "Epoch: 95/100, Batch: 2/5, Loss: 0.09943\n",
      "Epoch: 95/100, Batch: 3/5, Loss: 0.15418\n",
      "Epoch: 95/100, Batch: 4/5, Loss: 0.08196\n",
      "Epoch: 95/100, Batch: 5/5, Loss: 0.13571\n",
      "Epoch: 96/100, Batch: 1/5, Loss: 0.09189\n",
      "Epoch: 96/100, Batch: 2/5, Loss: 0.17139\n",
      "Epoch: 96/100, Batch: 3/5, Loss: 0.14288\n",
      "Epoch: 96/100, Batch: 4/5, Loss: 0.10368\n",
      "Epoch: 96/100, Batch: 5/5, Loss: 0.06756\n",
      "Epoch: 97/100, Batch: 1/5, Loss: 0.10443\n",
      "Epoch: 97/100, Batch: 2/5, Loss: 0.11896\n",
      "Epoch: 97/100, Batch: 3/5, Loss: 0.16132\n",
      "Epoch: 97/100, Batch: 4/5, Loss: 0.09991\n",
      "Epoch: 97/100, Batch: 5/5, Loss: 0.09129\n",
      "Epoch: 98/100, Batch: 1/5, Loss: 0.08786\n",
      "Epoch: 98/100, Batch: 2/5, Loss: 0.09753\n",
      "Epoch: 98/100, Batch: 3/5, Loss: 0.07738\n",
      "Epoch: 98/100, Batch: 4/5, Loss: 0.24946\n",
      "Epoch: 98/100, Batch: 5/5, Loss: 0.04838\n",
      "Epoch: 99/100, Batch: 1/5, Loss: 0.11661\n",
      "Epoch: 99/100, Batch: 2/5, Loss: 0.08811\n",
      "Epoch: 99/100, Batch: 3/5, Loss: 0.13006\n",
      "Epoch: 99/100, Batch: 4/5, Loss: 0.08181\n",
      "Epoch: 99/100, Batch: 5/5, Loss: 0.17029\n",
      "Epoch: 100/100, Batch: 1/5, Loss: 0.09693\n",
      "Epoch: 100/100, Batch: 2/5, Loss: 0.13022\n",
      "Epoch: 100/100, Batch: 3/5, Loss: 0.13064\n",
      "Epoch: 100/100, Batch: 4/5, Loss: 0.10577\n",
      "Epoch: 100/100, Batch: 5/5, Loss: 0.09552\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        X_train, y_train = samples\n",
    "        X_train.float() # RuntimeError: expected scalar type Float but found Double\n",
    "        \n",
    "        # H(x) 계산\n",
    "        X_pred = model(X_train)\n",
    "        \n",
    "        # loss 계산\n",
    "        # loss = torch.nn.CrossEntropyLoss(X_pred, y_train) 이런 식으로 한 줄로 작성하면 다음과 같은 에러 발생\n",
    "        # runtimeerror: boolean value of tensor with more than one value is ambiguous\n",
    "        criterion_label = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion_label(X_pred, y_train)\n",
    "        \n",
    "        # bp\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Epoch: {epoch}/{nb_epochs}, Batch: {batch_idx+1}/{len(dataloader)}, Loss: {round(loss.item(), 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct = 0\n",
    "for batch_idx, samples in enumerate(dataloader):\n",
    "    X, y = samples\n",
    "    X.float()\n",
    "    \n",
    "    X_pred = model(X)\n",
    "    \n",
    "    X_pred = torch.argmax(X_pred, dim=1)\n",
    "    \n",
    "    num_correct += sum(X_pred == y).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in all data:  0.987\n"
     ]
    }
   ],
   "source": [
    "accuracy = num_correct/len(dataset)\n",
    "print(\"Accuracy in all data: \", round(accuracy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 잘 학습된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. PyTorch에서 제공하는 데이터셋(e.g., CIFAR, MNIST, STL10)에는 transforms가 적용되는데 커스텀데이터셋에는 그런 어떻게 적용하나? `__init__`에서 따로 구현? 지원하는 함수가 따로 있을 것 같은데..."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[Week6] Class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
